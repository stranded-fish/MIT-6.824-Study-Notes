# Raft 论文阅读

目录：

- [Raft 论文阅读](#raft-论文阅读)
  - [摘要](#摘要)
  - [介绍](#介绍)
  - [复制状态机](#复制状态机)
  - [Paxos 存在的问题](#paxos-存在的问题)
  - [为可理解性而设计](#为可理解性而设计)
  - [Raft 共识算法](#raft-共识算法)
    - [Raft 基础](#raft-基础)
    - [Leader 选举](#leader-选举)
    - [日志复制](#日志复制)
    - [安全性](#安全性)
      - [选举限制](#选举限制)
      - [提交之前任期内的日志条目](#提交之前任期内的日志条目)
      - [安全性论证](#安全性论证)
    - [Follower 和 candidate 崩溃](#follower-和-candidate-崩溃)
    - [定时（timing）和可用性](#定时timing和可用性)
  - [集群成员变更](#集群成员变更)
  - [日志压缩](#日志压缩)
  - [客户端交互](#客户端交互)
  - [相关资料](#相关资料)

## 摘要

Raft 是一种基于日志复制的共识算法，其提供等价于 (multi-)Paxos 的效果。

Raft 通过将共识性的关键点拆分为以下三个部分：

* 选主
* 日志复制
* 安全性
* 实施更强的一致性以减少需要考虑的状态机状态

使得 Raft 相较于 Paxos 更加容易理解以及工程实现。

## 介绍

由于 Paxos 难以理解，并且其架构需要复杂的改变来支持实际的系统，所以以可理解性作为首要目标，设计了 Raft 共识算法。

Raft 共识算法通过以下两种技术来使其更容易理解：

* 分解法：Raft 分解为 选主、日志复制、安全；
* 状态空间简化：相较于 Paxos，Raft 减少了不确定性程度。
  
Raft 在许多方面类似于现有的一致性算法，但有以下新颖的特点：

* Strong leader：在 Raft 中，日志条目（log entries）只从 leader 流向其他服务器。 这简化了复制日志的管理，使得 raft 更容易理解。
* Leader election：Raft 使用随机计时器进行 leader 选举。 这只需在任何一致性算法都需要的心跳（heartbeats）上增加少量机制，同时能够简单快速地解决冲突。
* Membership changes：Raft 使用了一种新的联合一致性方法，其中两个不同配置的大多数在过渡期间重叠。 这允许集群在配置更改期间继续正常运行。

## 复制状态机

介绍了复制状态机的概念。

## Paxos 存在的问题

Paxos 主要存在两大缺点：

* 理论晦涩难懂；
* 不能简单的转化为工程实现。

## 为可理解性而设计

通过以下两大方法以提高可理解性：

1. 问题分解：将 Raft 算法分解为 选主、日志复制、安全 和 成员变更 4 个独立的问题；
2. 状态简化：减少需要考虑的状态的数量，使系统更加连贯，并且尽可能地消除不确定性。

## Raft 共识算法

Raft 会首先选出一个 leader，然后由 leader 全权负责管理日志来实现共识性。

* leader 从客户端接收日志条目；
* leader 将日志复制到其他服务器；
* 在保证安全性的情况下，通知其他服务器将日志条目应用到他们的状态机。

通过选举一个 leader 的方式，Raft 将共识问题分解成 3 个相对独立的子问题：

* Leader 选举：当前的 leader 宕机时，一个新的 leader 必须被选举出来。（章节 5.2）
* 日志复制：Leader 必须从客户端接收日志条目然后复制到集群中的其他节点，并且强制要求其他节点的日志和自己的保持一致。
* 安全性：Raft 中安全性的关键是图 3 中状态机的安全性：如果有任何的服务器节点已经应用了一个特定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一条不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；该解决方案在选举机制（5.2 节）上增加了额外的限制。

### Raft 基础

在任何一个时刻，每一个服务器节点都处于这三个状态之一：leader、follower 或 candidate。正常情况下，集群只有一个 leader 其他节点都是 follower。

* leader：处理所有客户端请求（如果客户端和 follower 通信，follower 会将请求重定向给 leader）；
* follower：总是被动的。不会发送任何请求，只是简单响应来自 leader 和 candidate 的请求；
* candidate：用于选举一个新的 leader。

Raft 将时间分割成任意长度的任期（term），任期用连续的整数标记。每一段任期从一次选举开始。Raft 保证了在任意一个任期内，最多只有一个 leader。

Raft 算法中服务器节点之间使用 RPC 进行通信，并且基本的共识算法只需要两种类型的 RPC：

* 请求投票（RequestVote RPC）由 candidate 在选举期间发起（章节 5.2）；
* 追加条目（AppendEntries RPC）由 leader 发起，用来复制日志和发送心跳（章节 5.3）。

第 7 节为了在服务器之间传输快照增加了第三种 安装快照 InstallSnapshot RPC。

当服务器没有及时的收到 RPC 的响应时，会进行重试，并且他们能够并行的发起 RPC 来获得最佳的性能。

### Leader 选举

Raft 使用一种心跳机制来触发 leader 选举。

当服务器程序启动时，他们都是 follower。

* 一个服务器节点只要能从 leader 或 candidate 处接收到有效的 RPC 就一直保持 follower 状态。
* Leader 周期性地向所有 follower 发送心跳（不包含日志条目的 AppendEntries RPC）来维持自己的地位。
* 如果一个 follower 在一段选举超时时间内没有接收到任何消息，它就假设系统中没有可用的 leader ，然后开始进行选举以选出新的 leader。

选举过程：

* follower 增加自己当前的任期号，并转化到 candidate 状态；
* 投票给自己，并且并行地向集群中地其他服务器节点发送 RequestVote RPC 拉选票；

Candidate 会一直保持当前状态直到以下三件事情之一发生：

* 它自己赢得了这次的选举（收到过半的投票）；
  * 当一个 candidate 获得集群中过半服务器节点针对同一个任期的投票，它就赢得了这次选举并成为 leader 。对于同一个任期，每个服务器节点只会投给一个 candidate ，按照先来先服务（first-come-first-served）的原则（注意：5.4 节在投票上增加了额外的限制）。要求获得过半投票的规则确保了最多只有一个 candidate 赢得此次选举（图 3 中的选举安全性）。一旦 candidate 赢得选举，就立即成为 leader 。然后它会向其他的服务器节点发送心跳消息来确定自己的地位并阻止新的选举。
* 其他的服务器节点成为 leader；
  * 在等待投票期间，candidate 可能会收到另一个声称自己是 leader 的服务器节点发来的 AppendEntries RPC 。如果这个 leader 的任期号（包含在RPC中）不小于 candidate 当前的任期号，那么 candidate 会承认该 leader 的合法地位并回到 follower 状态。 如果 RPC 中的任期号比自己的小，那么 candidate 就会拒绝这次的 RPC 并且继续保持 candidate 状态。
* 一段时间之后没有任何获胜者。
  * 如果有多个 follower 同时成为 candidate ，那么选票可能会被瓜分以至于没有 candidate 赢得过半的投票。当这种情况发生时，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，如果没有其他机制的话，该情况可能会无限重复。

Raft 算法使用随机选举超时时间的方法来确保很少发生选票瓜分的情况，就算发生也能很快地解决。

* 为了阻止选票一开始就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后该服务器赢得选举并在其他服务器超时之前发送心跳。
* 同样的机制被用来解决选票被瓜分的情况。每个 candidate 在开始一次选举的时候会重置一个随机的选举超时时间，然后一直等待直到选举超时；这样减小了在新的选举中再次发生选票瓜分情况的可能性。

### 日志复制

当 leader 被选举出来，就开始为客户端请求提供服务：

* leader 将指令（来自客户端的请求）作为一个新的条目追加到日志中去，然后并行的发起 AppendEntries RPC 给其他的服务器，让它们复制该条目；
* 当该条目被安全地复制，leader 会应用该条目到它的状态机（状态机执行该指令）然后把执行地结果返回给客户端。如果 follower 崩溃或者运行缓慢，或者网络丢包，领导人会不断地重试 AppendEntries RPC（即使已经回复了客户端）直到所有的 follower 最终都存储了所有的日志条目。

日志条目的组织形式：

* 一条状态机指令；
* leader 收到该指令时的任期号。

leader 决定什么时候将日志条目应用到状态机中是安全的，这种日志条目被称为是已提交的。

* Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。
* 一旦创建该日志条目的 leader 将它复制到过半的服务器上，该日志条目就会被提交。同时，leader 日志中该日志条目之前的所有日志条目也都会被提交，包括由其他 leader 创建的条目。
* Leader 追踪将会被提交的日志条目的最大索引，未来的所有 AppendEntries RPC 都会包含该索引，这样其他的服务器才能最终知道哪些日志条目需要被提交。
* Follower 一旦知道某个日志条目已经被提交就会将该日志条目应用到自己的本地状态机中（按照日志的顺序）。

Raft 日志匹配特性：

* 如果不同日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。
* 如果不同日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也都相同。

由于节点可能发生崩溃，并导致日志处于不一致的状态。

解决方法：Raft 算法中，leader 通过强制 follower 复制它的日志来解决不一致问题。即，follower 中跟 leader 冲突的日志条目会被 leader 的日志条目覆盖。

实现方法： 通过 leader 对每一个 follower 维护一个 nextIndex，并通过 AppendEntries RPCs 中的一致性检查的回复，来找到两者达成一致的最大的日志条目，将 follower 中跟 leader 冲突的日志条目全部删除然后追加 leader 中的日志条目（如果有需要追加的日志条目的话）。一旦 AppendEntries RPC 成功，follower 的日志就和 leader 一致，并且在该任期接下来的时间里保持一致。

### 安全性

#### 选举限制

Raft 保证新 leader 在当选时就包含了之前所有任期号中已经提交的日志条目。

实现方法：通过投票的方式阻止 candidate 赢得选举除非该 candidate 包含了所有已经提交的日志条目。

原理：候选人为了赢得选举必须与集群中的过半节点通信，这意味着至少其中一个服务器节点包含了所有已提交的日志条目。如果 candidate 的日志至少和过半的服务器节点一样新（由最后一条日志的索引值和任期号定义），那么他一定包含了所有已经提交的日志条目。

#### 提交之前任期内的日志条目

问题：如果 leader（currentTerm = term1）可以直接提交 term < term1 的 entry（也即，之前任期内已经存储到过半服务器上的日志条目），那么可能导致已经 commit 的 entry 被重写。

解决方法：Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目，只有 leader 当前任期内的日志条目才通过计算副本数目的方式来提交；也即，不允许 leader（currentTerm = term2），直接提交 term < term2 的 entry，只允许提交term = term2 的 entry，一旦当前任期的某个日志条目被提交，那么由于日志匹配特性，之前的所有日志条目也都会被间接地提交。

#### 安全性论证

通过做出假设：任期 T 的 leader（leader T）在任期内提交了一个日志条目，但是该日志条目没有被存储到未来某些任期的 leader 中，并推出其矛盾，来证明 Leader Completeness 特性。

Leader Completeness：如果在给定的任期中提交了一个日志条目，那么该条目将出现在所有更高任期的 leader 日志中。

### Follower 和 candidate 崩溃

follower 和 candidate 崩溃后的处理方式相同。如果 follower 或 candidate 崩溃了，那么后续发送给他们的 RequestVote 和 AppendEntries RPCs 都会失败。Raft 通过无限的重试来处理这种失败；同时，Raft 的 RPCs 是幂等的，重试不会造成伤害。

### 定时（timing）和可用性

可用性（系统能够即使响应客户端）不可避免的要依赖于定时。Leader 选举是 Raft 中定时最为关键的方面。 只要整个系统满足下面的时间要求，Raft 就可以选举出并维持一个稳定的 leader：

> 广播时间（broadcastTime） << 选举超时时间（electionTimeout） << 平均故障间隔时间（MTBF）

* 广播时间：一个服务器并行地发送 RPCs 给集群中所有的其他服务器并接收到响应的平均时间；
  * 系统决定，大约在 0.5 ms 到 20 ms 之间；
  * 广播时间需要比选举超时时间小一个量级：
    * 只有这样 leader 才能够可靠地发送心跳消息来阻止 follower 进入选举状态；
    * 配合随机化选举超时时间的方法，使得选票瓜分情况变得不可能。
* 选举超时时间：选举超时时间是从一个固定的区间内随机选择。每个 candidate 在开始一次选举的时候会重置一个随机的选举超时时间，然后一直等待直到选举超时；
  * 自定义，大约在 10 ms 到 500 ms 之间；
  * 选举超时时间需要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定地运行。（因为 leader 崩溃后，系统在选举超时时间内将不可用，故希望该时间只占整个时间的一小部分）
* 平均故障间隔时间：对于一台服务器而言，两次故障间隔时间的平均值。
  * 系统决定，大约在几个月甚至更长；

## 集群成员变更

上面我们提到单节点的成员变更，很多时候这已经能满足我们的需求了，但是有些时候我们可能会需要随意的的集群成员变更，每次变更多个节点，那么我们就需要Raft的Joint Consensus, 尽管这会引入很多的复杂性。

Joint Consensus会将集群的配置转换到一个临时状态，然后开始变更：

1. Leader收到Cnew的成员变更请求，然后生成一个Cold,new的ConfChang日志，马上应用该日志，然后将日志通过AppendEntries请求复制到Follower中，收到该ConfChange的节点马上应用该配置作为当前节点的配置
2. 在将Cold,new日志复制到大多数节点上时，那么Cold,new的日志就可以提交了，在Cold,new的ConfChange日志被提交以后，马上创建一个Cnew的ConfChange的日志，并将该日志通过AppendEntries请求复制到Follower中，收到该ConfChange的节点马上应用该配置作为当前节点的配置
3. 一旦Cnew的日志复制到大多数节点上时，那么Cnew的日志就可以提交了，在Cnew日志提交以后，就可以开始下一轮的成员变更了

为了理解上面的流程，我们有几个概念需要解释一下：

* Cold,new：这个配置是指Cold，和Cnew的联合配置，其值为Cold和Cnew的配置的交集，比如Cold为[A, B, C]， Cnew为[B, C, D]，那么Cold,new就为[A, B, C, D]
* Cold,new的大多数：是指Cold中的大多数和Cnew中的大多数，如下表所示，第一列因为Cnew的C, D没有Replicate到日志，所以并不能达到一致

Cold    | Cnew    | Replicate 结果 | 是否是Majority
--------|---------|----------------|------------
A, B, C | B, C, D | A+, B+, C-, D- | 否
A, B, C | B, C, D | A+, B+, C+, D- | 是
A, B, C | B, C, D | A-, B+, C+, D- | 是

由上可以看出，整个集群的变更分为几个过渡期，就如下图所示，在每一个时期，每一个任期下都不可能出现两个Leader：

1. Cold,new日志在提交之前，在这个阶段，Cold,new中的所有节点有可能处于Cold的配置下，也有可能处于Cold,new的配置下，如果这个时候原Leader宕机了，无论是发起新一轮投票的节点当前的配置是Cold还是Cold,new，都需要Cold的节点同意投票，所以不会出现两个Leader
2. Cold,new提交之后，Cnew下发之前，此时所有Cold,new的配置已经在Cold和Cnew的大多数节点上，如果集群中的节点超时，那么肯定只有有Cold,new配置的节点才能成为Leader，所以不会出现两个Leader
3. Cnew下发以后，Cnew提交之前，此时集群中的节点可能有三种，Cold的节点（可能一直没有收到请求）， Cold,new的节点，Cnew的节点，其中Cold的节点因为没有最新的日志的，集群中的大多数节点是不会给他投票的，剩下的持有Cnew和Cold,new的节点，无论是谁发起选举，都需要Cnew同意，那么也是不会出现两个Leader
4. Cnew提交之后，这个时候集群处于Cnew配置下运行，只有Cnew的节点才可以成为Leader，这个时候就可以开始下一轮的成员变更了

## 日志压缩

Raft 的日志在正常操作中随着包含更多的客户端请求会不断地增长，而日志增长会带来许多问题：

* 占用越来越多的空间；
* 需要花更多的时间来回放。

日志压缩主要有两种方法：

* 快照压缩：将当前系统的状态都以快照形式持久化到稳定的存储中，该时间点之前的日志全部丢弃。
* 增量压缩：每次只对一小部分数据进行操作，分散了压缩的负载压力。

而增量压缩技术相比快照技术而言，需要额外的机制和复杂度。

Raft 日志压缩的思想：

每个服务器独立地创建快照（不依赖 leader），快照只包括自己日志中已经被提交的日志条目；

为了支持快照后的第一个条目 AppendEntries 一致性检查，快照会保留一些元数据：

* the last included index：指的是最后一个被快照取代的日志条目的索引值（状态机最后应用的日志条目）；
* the last included term：是该条目的任期号。

同时为了支持集群成员变更，快照中也会包括日志中最新的配置。

一旦服务器完成写快照，他就可以删除 last included index 之前的所有日志条目，包括之前的快照。

除了各个服务器独立地创建快照，leader 也会偶尔发送快照给一些落后的 follower，这种情况通常发生在 leader 已经丢弃了需要发送给 follower 的下一条日志条目（一般不会发生，因为一般是同步的，只有比如一个例外运行缓慢的 follower 或者 新加入集群的服务器）

有两个问题会影响快照的性能：

* 快照创建的频率：如果快照创建过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，就要承担耗尽存储容量的风险，同时也增加了重启时日志回放的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置得显著大于期望的快照的大小，那么快照的磁盘带宽负载就会很小。
* 写入快照的耗时：并且我们不希望它影响到正常的操作。解决方案是通过写时复制的技术，这样新的更新就可以在不影响正在写的快照的情况下被接收。

## 客户端交互

客户端交互流程：

* 当客户端第一次启动的时候，它会随机挑选一个服务器  进行通信。
  * 如果客户端第一次挑选的服务器不是 leader ，那么该服务器会拒绝客户端的请求并且提供关于它最近接收到的领导人的信息（AppendEntries 请求包含了 leader 的网络地址）。
  * 如果 leader 已经崩溃了，客户端请求就会超时；客户端之后会再次随机挑选服务器进行重试。

Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在它的调用和回复之间）。但是，如上述，Raft 可能执行同一条命令多次：例如，如果 leader 在提交了该日志条目之后，响应客户端之前崩溃了，那么客户端会和新的 leader 重试这条指令，导致这条命令被再次执行。

解决方案：客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每个客户端已经处理的最新的序列号以及相关联的回复。如果接收到一条指令，该指令的序列号已经被执行过了，就立即返回结果，而不重新执行该请求。

只读的操作 ？？？ todo

## 相关资料
